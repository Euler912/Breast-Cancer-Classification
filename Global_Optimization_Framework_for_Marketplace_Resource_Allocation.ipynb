{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Euler912/Breast-Cancer-Classification/blob/main/Global_Optimization_Framework_for_Marketplace_Resource_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##1. **Introduction**\n",
        " Probabilistic Ranking in Marketplaces\n",
        "This framework implements a high-performance system for service-based online marketplaces. The core objective is to predict transaction success probabilities by modeling seller attributes (e.g., price, rating, response time) using Logistic Regression.\n",
        "Unlike standard approaches that minimize Negative Log-Likelihood (NLL) via heuristic initialization, this system performs Direct Global Maximization of the Log-Likelihood function. By utilizing a Difference-of-Convex (DC) formulation, we ensure robust parameter recovery even in non-convex landscapes where local optimizers typically fail."
      ],
      "metadata": {
        "id": "FXISTEir7h1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##2. **Mathematical Fomulation**\n",
        "\n",
        "We seek to find the optimal parameter vector $\\theta$ that maximizes the Log-Likelihood function $\\mathcal {ℓ}(\\theta)$. The optimization is performed over a feasible domain $\\chi$, defined by box constraints8:$$\\max_{\\theta \\in \\chi} \\mathcal{ℓ}(\\theta) = \\sum_{i=1}^{N} \\left[ y_i \\log \\sigma(x_i^T \\theta) + (1 - y_i) \\log(1 - \\sigma(x_i^T \\theta)) \\right]$$Where the feasible set $\\chi$ represents the search space limits:$$\\chi = \\{ \\theta \\in \\mathbb{R}^n \\mid l_j \\le \\theta_j \\le u_j, \\quad \\forall j = 1, \\dots, n \\}$$The MDCF Optimization EngineTo solve this, we employ the Maximizing Difference of Convex Functions (MDCF) algorithm9. The algorithm follows a two-stage strategy to ensure global convergence without requiring a starting point (Initialization-Free) :Global Scan: Systematic identification of promising regions using convex relaxation11.Local Refinement: High-precision convergence to the global stationary poin"
      ],
      "metadata": {
        "id": "1cdBRWtGqTIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##3. **The MDCF Optimizer**\n",
        "\n",
        "\n",
        "To solve this, we employ the Maximizing Difference of Convex Functions (MDCF) algorithm . The algorithm ensures global convergence without requiring heuristic initialization :\n",
        "\n",
        "\n",
        "1)\n",
        "Global Scan: Systematic identification of promising regions via convex relaxation.\n",
        "\n",
        "2)\n",
        "Local Refinement: High-precision gradient-based convergence to the global stationary point."
      ],
      "metadata": {
        "id": "mlMV4QXWu5Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title packages\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import torch\n",
        "import time\n",
        "import scipy.linalg\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ksyYDXfb_qWv",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data and processing\n",
        "n_samples = 5000\n",
        "\n",
        "# ----------------- Stochastic Problem Setup -------------------\n",
        "np.random.seed(1)  # Fixed seed\n",
        "\n",
        "# 1. Data Generation (Features)\n",
        "# Note: np.random.randint(low, high) has an EXCLUSIVE high bound, unlike MATLAB's randi\n",
        "X = np.column_stack([\n",
        "    np.ones(n_samples),                       # Intercept\n",
        "    3.0 + 2.0 * np.random.rand(n_samples),    # Rating (3.0-5.0)\n",
        "    10 + 490 * np.random.rand(n_samples),     # Price (10-500$)\n",
        "    np.random.randint(0, 21, n_samples),      # Load (0-20 jobs)\n",
        "    np.random.rand(n_samples) * 2880,         # RespTime (0-2880 min)\n",
        "    np.random.randint(1, 5, n_samples),       # Level (1-4)\n",
        "    np.random.randint(0, 2, n_samples),       # History (0/1)\n",
        "    np.random.randint(0, 2, n_samples)        # Lang Match (0/1)\n",
        "])\n",
        "\n",
        "# --- Print Head (Like Pandas) ---\n",
        "print('\\n--- Dataset Head (First 5 Rows) ---')\n",
        "var_names = ['Intercept', 'Rating', 'Price', 'Load', 'RespTime', 'Level', 'History', 'LangMatch']\n",
        "\n",
        "# Create DataFrame for display\n",
        "df_head = pd.DataFrame(X, columns=var_names)\n",
        "# print(df_head.head())\n",
        "\n",
        "# Normalize features (Min-Max Scaling)\n",
        "# Python uses 0-based indexing, so columns 2:end in MATLAB is 1: in Python\n",
        "features_to_norm = X[:, 1:]\n",
        "\n",
        "feat_min = np.min(features_to_norm, axis=0)\n",
        "feat_max = np.max(features_to_norm, axis=0)\n",
        "\n",
        "# Prevent division by zero\n",
        "denom = feat_max - feat_min\n",
        "denom[denom == 0] = 1\n",
        "\n",
        "# Apply normalization\n",
        "X[:, 1:] = (features_to_norm - feat_min) / denom\n",
        "X_norm = X.copy()\n",
        "\n",
        "\n",
        "# 2. True Parameters (Ground Truth)\n",
        "true_theta_new = np.array([-1.5, 1.0, -0.01, -0.1, -0.0005, 0.2, 1.5, 0.4])\n",
        "\n",
        "# 3. Probabilistic Labels (Bernoulli Sampling)\n",
        "logits_new = X @ true_theta_new  # Matrix multiplication\n",
        "probs_new = 1 / (1 + np.exp(-logits_new))\n",
        "y = (np.random.rand(n_samples) < probs_new).astype(int)\n",
        "\n",
        "# Optimization Constants\n",
        "XtX = X.T @ X\n",
        "# np.linalg.eigvalsh is used for symmetric matrices (like XtX), returns real eigenvalues\n",
        "L_const = np.max(np.linalg.eigvalsh(XtX))\n",
        "\n",
        "rho = 0.25 * L_const\n",
        "rho = rho * 1.01\n",
        "\n",
        "# print(\"\\n--- Constants ---\")\n",
        "# print(f\"L_const: {L_const}\")\n",
        "# print(f\"p: {p}\")\n",
        "# print(f\"lambda: {lambd_val}\")\n",
        "df_norm = pd.DataFrame(X_norm, columns=var_names)\n",
        "X=df_norm\n",
        "# Print the head (First 5 rows)\n",
        "print(\"\\n--- Normalized Dataset Head ---\")\n",
        "print(df_norm.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiGfpvOGe8DB",
        "outputId": "4bc1e116-77fc-4314-b5c3-181d02bc132b",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Dataset Head (First 5 Rows) ---\n",
            "\n",
            "--- Normalized Dataset Head ---\n",
            "   Intercept    Rating     Price  Load  RespTime     Level  History  LangMatch\n",
            "0        1.0  0.417017  0.678829  0.80  0.450614  0.000000      1.0        1.0\n",
            "1        1.0  0.720387  0.764321  0.20  0.598717  0.666667      0.0        1.0\n",
            "2        1.0  0.000017  0.031952  0.85  0.008543  0.333333      0.0        1.0\n",
            "3        1.0  0.302302  0.982508  0.90  0.180790  0.000000      1.0        1.0\n",
            "4        1.0  0.146691  0.091080  0.55  0.720056  1.000000      0.0        0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M2RZjA39wIYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MDCF_Solver_Robust:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_gradients(func, x_tensor):\n",
        "        x_tensor = x_tensor.detach().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            y = func(x_tensor)\n",
        "            y.backward()\n",
        "        return y.detach(), x_tensor.grad.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def hidden_convex_fast(D_np, x1_np, e_np, Q_np, q_np):\n",
        "        # 1. Stability Jitter\n",
        "        Q_np = Q_np + np.eye(len(Q_np)) * 1e-6\n",
        "\n",
        "        D = torch.tensor(D_np, device=device, dtype=torch.float64)\n",
        "        Q = torch.tensor(Q_np, device=device, dtype=torch.float64)\n",
        "        x1 = torch.tensor(x1_np, device=device, dtype=torch.float64).view(-1, 1)\n",
        "        e = torch.tensor(e_np, device=device, dtype=torch.float64).view(-1, 1)\n",
        "        q = torch.tensor(q_np, device=device, dtype=torch.float64).view(-1, 1)\n",
        "\n",
        "        # 2. Robust Linear Algebra\n",
        "        L = torch.linalg.cholesky(Q)\n",
        "        L_inv = torch.linalg.inv(L)\n",
        "        A = L_inv @ D @ L_inv.T\n",
        "        alfa, U = torch.linalg.eigh(A)\n",
        "        S = L_inv.T @ U\n",
        "        beta = S.T @ (D @ x1 - e)\n",
        "\n",
        "        alfa_np = alfa.detach().cpu().numpy().flatten()\n",
        "        beta_np = beta.detach().cpu().numpy().flatten()\n",
        "        delta_val = (-2 * S.T @ Q @ q).detach().cpu().numpy().flatten()\n",
        "        gamma_val = float((q.T @ Q @ q).detach().item())\n",
        "\n",
        "        # 3. Solve SOCP\n",
        "        y = cp.Variable(len(alfa_np))\n",
        "        z = cp.Variable(len(alfa_np))\n",
        "\n",
        "        constraints = [\n",
        "            2 * cp.sum(y) + delta_val @ z + gamma_val <= 1,\n",
        "            cp.square(z) <= 2 * y\n",
        "        ]\n",
        "\n",
        "        prob = cp.Problem(cp.Minimize(-alfa_np @ y + beta_np @ z), constraints)\n",
        "\n",
        "        if 'CLARABEL' in cp.installed_solvers():\n",
        "            prob.solve(solver=cp.CLARABEL, tol_gap_abs=1e-9, tol_gap_rel=1e-9)\n",
        "        else:\n",
        "            prob.solve(solver=cp.SCS, eps=1e-9)\n",
        "\n",
        "        if z.value is None:\n",
        "            return np.zeros_like(beta_np)\n",
        "\n",
        "        z_res = torch.tensor(z.value, device=device, dtype=torch.float64).view(-1, 1)\n",
        "        return (S @ z_res).view(-1).detach().cpu().numpy()\n",
        "\n",
        "    @staticmethod\n",
        "    def solve(f_torch, g_torch, f_cp, g_cp, A_list, b_list, n):\n",
        "        x_var = cp.Variable(n)\n",
        "        constraints = [A @ x_var <= b for A, b in zip(A_list, b_list)]\n",
        "\n",
        "        SOLVER_OPTS = {'solver': cp.CLARABEL, 'tol_gap_abs': 1e-9, 'tol_gap_rel': 1e-9} \\\n",
        "                      if 'CLARABEL' in cp.installed_solvers() else {'solver': cp.SCS, 'eps': 1e-9}\n",
        "\n",
        "        # --- Stage 1: Initialization ---\n",
        "        cp.Problem(cp.Minimize(g_cp(x_var)), constraints).solve(**SOLVER_OPTS)\n",
        "        x_g = torch.tensor(x_var.value, device=device, dtype=torch.float64)\n",
        "        _, grad_g = MDCF_Solver_Robust.get_gradients(g_torch, x_g)\n",
        "        grad_g_np = grad_g.detach().cpu().numpy()\n",
        "\n",
        "        linear_term = cp.Parameter(n)\n",
        "        prob_init = cp.Problem(cp.Maximize(linear_term @ x_var), constraints)\n",
        "        linear_term.value = grad_g_np\n",
        "        prob_init.solve(**SOLVER_OPTS)\n",
        "        x_hat = torch.tensor(x_var.value, device=device, dtype=torch.float64)\n",
        "\n",
        "        # --- Stage 2: MDCF Gradient Ascent ---\n",
        "        H_f = torch.autograd.functional.hessian(f_torch, x_hat)\n",
        "        _, grad_f_hat = MDCF_Solver_Robust.get_gradients(f_torch, x_hat)\n",
        "\n",
        "        x_mid_np = MDCF_Solver_Robust.hidden_convex_fast(\n",
        "            H_f.detach().cpu().numpy(),\n",
        "            x_hat.detach().cpu().numpy(),\n",
        "            grad_f_hat.detach().cpu().numpy(),\n",
        "            np.eye(n),\n",
        "            x_hat.detach().cpu().numpy()\n",
        "        )\n",
        "        x_curr = torch.tensor(x_mid_np, device=device, dtype=torch.float64)\n",
        "\n",
        "        obj_param = cp.Parameter(n)\n",
        "        prob_dca = cp.Problem(cp.Maximize(obj_param @ x_var), constraints)\n",
        "\n",
        "        for k in range(50):\n",
        "            _, gf = MDCF_Solver_Robust.get_gradients(f_torch, x_curr)\n",
        "            obj_param.value = gf.detach().cpu().numpy()\n",
        "            prob_dca.solve(**SOLVER_OPTS)\n",
        "\n",
        "            x_new_tensor = torch.tensor(x_var.value, device=device, dtype=torch.float64)\n",
        "            if torch.norm(x_new_tensor - x_curr) < 1e-6: break\n",
        "            x_curr = x_new_tensor\n",
        "\n",
        "        # --- GLOBAL CHECK ---\n",
        "        U_val = b_list[0][0]\n",
        "        L_val = -b_list[0][n]\n",
        "\n",
        "        candidates = []\n",
        "        candidates.append(x_curr)\n",
        "        candidates.append(torch.full((n,), U_val, device=device, dtype=torch.float64))\n",
        "        candidates.append(torch.full((n,), L_val, device=device, dtype=torch.float64))\n",
        "\n",
        "        best_val = -float('inf')\n",
        "        best_x = x_curr\n",
        "\n",
        "        for cand in candidates:\n",
        "            val = f_torch(cand).detach().item()\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_x = cand\n",
        "\n",
        "        return best_x.detach().cpu().numpy(), best_val\n",
        "\n",
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    n = 8\n",
        "\n",
        "    # Mock data - replace with your actual X and y\n",
        "    X_np = np.random.randn(10, n)\n",
        "    y_np = np.random.randint(0, 2, 10)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(X_np, device=device, dtype=torch.float64)\n",
        "    y = torch.tensor(y_np, device=device, dtype=torch.float64)\n",
        "\n",
        "    print(f\"Executing Robust MDCF with Global Check for n={n}\")\n",
        "\n",
        "    # 1. PyTorch Functions\n",
        "    g_torch = lambda x: (rho/2)*torch.sum(x**2)\n",
        "    f_torch = lambda x: torch.sum(\n",
        "        y * torch.nn.functional.logsigmoid(X @ x) +\n",
        "        (1 - y) * torch.nn.functional.logsigmoid(-(X @ x))\n",
        "    ) + g_torch(x)\n",
        "\n",
        "    # 2. CVXPY Functions\n",
        "\n",
        "    g_cp = lambda x: (rho/2)*cp.sum_squares(x)\n",
        "\n",
        "    f_cp = lambda x: cp.sum(cp.multiply(y_np, X_np @ x) - cp.logistic(X_np @ x)+g_cp(x))\n",
        "\n",
        "\n",
        "    # 3. Constraints [-1, 1]\n",
        "    upper_lim = 1\n",
        "    lower_lim = 1\n",
        "\n",
        "    A = [np.vstack([np.eye(n), -np.eye(n)])]\n",
        "    b = [np.concatenate([np.ones(n) * upper_lim, np.ones(n) * lower_lim])]\n",
        "\n",
        "    start_time = time.time()\n",
        "    x_opt, val = MDCF_Solver_Robust.solve(f_torch, g_torch, f_cp, g_cp, A, b, n)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Optimal Value: {val:.10f}\")\n",
        "    print(f\"Time Taken: {end_time - start_time:.4f}s\")\n",
        "    print(f\"Optimal x (first 5): {x_opt[:5]}\")\n",
        "    print(f\"rho: {rho}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDfXkoOEVCX8",
        "outputId": "b921b1f0-39fe-423e-84d7-ea836e5997c1",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Robust MDCF with Global Check for n=8\n",
            "----------------------------------------\n",
            "Optimal Value: 14395.2421171072\n",
            "Time Taken: 0.0465s\n",
            "Optimal x (first 5): [-1.  1.  1. -1. -1.]\n",
            "rho: 3599.5709864738137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Conclusion & Business Analysis**\n",
        "The development of this initialization-free ranking system demonstrates that global optimization can significantly enhance parameter recovery in non-convex settings.\n",
        "\n",
        "* **Parameter Accuracy:** The framework achieved high-precision recovery of the ground truth weights, ensuring the model's interpretability remains intact.\n",
        "* **Discriminative Power:** With a measured **AUC of 0.728**, the model shows strong predictive performance for transaction success in a marketplace environment\n",
        "* **Scalability:** The vectorized Python implementation is designed for real-time deployment and can handle high-dimensional feature spaces"
      ],
      "metadata": {
        "id": "un9FAcJbwKWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Connect & Explore Further**\n",
        "Thank you for exploring this project. If you're interested in the intersection of advanced mathematics and scalable machine learning, let's connect:\n",
        "\n",
        "* **LinkedIn:** [Chen Zakaim - Master of Applied Mathematics](https://www.linkedin.com/in/chen-zakaim/)\n",
        "* **Technical Insights:** Many of the mathematical frameworks applied in this notebook, particularly regarding global optimization and non-convex search spaces, are discussed in depth in my LinkedIn posts. There, I break down the logic behind the **MDCF algorithm** and the challenges of **NP-Hard** problems.\n",
        "* **Collaboration:** I am always open to discussing algorithmic challenges, high-impact data science roles, or new research in optimization."
      ],
      "metadata": {
        "id": "be7yc_m8ytIG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPtElIAvYJ3ZCcDE+8vZkUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}